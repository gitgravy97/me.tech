<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <!-- The self-made style sheet import -->
    <link rel="stylesheet" type="text/css" href="css/stylish.css">
    <link rel="stylesheet" type="text/css" href="css/print.css">
    <link rel="shortcut icon" href="images/favicon.png" type="image/x-icon">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-138259283-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'UA-138259283-1');
    </script>
    <title>Projects & Visualizations</title> 
</head>
<body>
    <header>
        <nav>
            <a class="skip-to-cont" href=#content>Skip to Content</a>
            <ul>
                <li><a href="index.html">Home</a></li>
                <li><a href="p2_education.html">Education</a></li>
                <li><a href="p3_employment.html">Work Experience</a></li>
                <li><a href="p4_skills.html">Skills, Interests, & Aspirations</a></li>
                <!--<li><a href="p5_contact.html">Contact</a></li>-->
                <li><a href="p6_gallery.html">Gallery</a></li>
                <li><a href="p7_projects.html" class="current">Projects & Visualizations</a></li>
            </ul>
        </nav>
    <h1 class="header-title">Projects & Visualizations</h1>
    </header>

    <main id="content">
    <h1 class="project-section">Active / In-Development Projects</h1>

    <div class="project-entry">
        <h2>Wolverine Watchdog</h2>
        <p><b>What:</b> Wolverine Watchdog started as a COVID-summer aspiration to teach myself more about modern web-development and is 
        effectively a platform intended to help users be able to more-flexibly query, access, map, and explore a large corpus of crime /
        "fire and safety" data published about incidents on the properties of the University of Michigan's Ann Arbor campuses. The data 
        ranges back to January of 2000, but was initially provided on a far more cumbersome and restrictive platform. I am in the process
        of gathering the data into a MongoDB and making it more accessible and interactive for members of the community that may be able 
        to extrapolate greater utility from this local, interesting, safety-oriented dataset.</p>
        <p><b>Why:</b> I'm chronically fascinated with legal and criminal studies, love the campus community, and remember mentioning
        the thought of this project once to a colleague working in the Housing/dormitory staff, who seemed genuinely both interested 
        and excited at the idea of something like this existing, so now I'm working on actually making it.</p>
        <p><b>How:</b> React for front-end, Node for the back-end, Express for API work, and MongoDB for storage.</p>
        <p><b>Can I See It:</b> Yeah, though development's been on hiatus due to school, I'm planning to deploy publically in
            either December 2020 or January 2021</p>
    </div>
    <hr>

    <div class="project-entry">
        <h2>Git-Job</h2>
        <p><b>What:</b> Co-developing a specialized search engine tailored to helping one find tech-sector jobs that better match their 
        qualifications, both based on what they report having, and what they report not having. We hope to distinguish our engine by
        doing more intense feature-extraction from job-posting descriptions, providing novel filter options to end-users, and a unique
        query operator.</p>
        <p><b>Why:</b> Colleagues and I grow tired of excitedly reading cool "Entry Level" jobs that ask for require criteria in the end that
        one can't simply filter out in other job-search engines, such as needing a PhD, familiarity with a specific language, or citizenship.</p>
        <p><b>How:</b> We're in the earlier phases currently and doing a lot of exploratory work in Python. Ultimately, we're 
        hoping for this to be a live web application. We've done enough literature review to realize just how many unique and interesting approaches
        there can be within this space, especially in ranking methodology and in feature extraction.</p>
        <p><b>When Can I See It:</b> We're hoping to have this as a live web application by December 2020. We're sourcing the job posting data from
        another platform and are trying to figure out if we can reasonably use this on live data all the way or if we'll have to constrict it to a large
        corpus of more static data as just a proof-of-concept, which would definitely be easier but less-than-ideal, and frankly less fun.</p>
    </div>
    <hr>

    <div class="project-entry">
        <h2>Exploring Consumer Review E-Fraud</h2>
        <p><b>What:</b> A colleague and I are working on mining a corpus of user-sourced product reviews provided by a major ecommerce platform,
            which are labelled as either legitimate or fraudulent. We are looking to find patterns in the textual composition of real and fake
            reviews to assist with training fraud-detection models to recognize e-commerce product/service-review fraud.</p>
        <p><b>Why:</b> Moreso now than in prior years, anecdotally, I've been observing more bots than ever before on social platforms, and seeing 
            more allegations on consumer e-commerce platforms / review systems of falsified reviews. Considering how important e-commerce is, 
        especially in this COVID19 era where people engage with online shopping more than ever before, this seemed not only an interesting challenge,
        but a potentially impactful line of study.</p>
        <p><b>How:</b> We're currently working on that, but initially, a lot of literature review. More often than not, it seems literature in this
        space focuses on a blend of textual analysis with user-behavioral / metadata analysis, however we only have textual data at our disposal, so
        we are focusing heavily on feature extraction and comparison along many, many features in this early phase. Updates to come.</p>
    </div>
    <hr>

    <div class="project-entry">
        <h2>PNC Bank - Product-Centric Qualitative Consulting</h2>
        <p><b>What:</b> With a team of other students in my consulting course, we are working (under NDA) in order to propose carefully-thought-out
            and well-contextualized recommendations to guide the continued improvement of a proprietary client-facing portal.</p>
        <p><b>Why:</b> Desire to experience facillitating better user-experience development in a significant client-facing product</p>
        <p><b>How:</b> Largely, a combination of literature review / background research, conducting a series of qualitative interviews, careful notetaking,
        and the modeling of systemic relationships and processes</p>
    </div>
    <hr>

    <h1 class="project-section">Former Projects</h1>
    
    <div class="project-entry">

    <h2>Senior Capstone Project -- Unemployment Tracker</h2>
    <p>
        <b>What:</b> Working with a team of two other graduating seniors in the informatics program's data analytics subconcentration, we were assigned a client and final project.
        I can't go super in-depth due to the NDA, but we spent many hours working on cleaning and processing a massive amount of data regarding unemployment insurance, claims, and claimancy protests to build insight systems for internal use.
    </p>
    <p><b>Why:</b> I've always been interested in law and business, have had a growing interest in finance and insurance, and selecting this client seemed a great way to apply my technical skills to one of my other veins of interest</p>
    <p><b>
        How:</b> A mix of Pandas, statistics, sklearn for machine learning purposes, and so, so, so so so much RegEx / Regular Expression work... so many regular expressions...
    </p>
    </div>

    <hr>

    <div class="project-entry">
    <h2>Kaggle Competition -- San Francisco Crime Classification</h2>
    <p>
        <b>What:</b> Working with four classmates in our Exploratory Data Analysis class, we built a project for the 
        <a href="https://www.kaggle.com/c/sf-crime/data">San Francisco Crime Classification</a> Kaggle competition.<br>
        In addition to a ML classifier that can predict the category of a given crime (from a pool of 39 established categories) based
        largely on temporal and geolocational data, we have built out interactive maps to visualize the data.
    </p>
    <p><b>Why:</b> Group selection-by-vote and a rather mutual interest in criminal data</p>
    <p><b>
        How:</b> The data was cleaned primarily with Pandas. Our classifier of choice ended up being the SKLearn's Random Forest Classifier. The visualization component was build with Folium.
    </p>
    <p><b>Interactive Experience:</b> <a href="http://www.si370finalproject.xyz">Hosted Here</a></p>
    </div>

    <hr>

    <div class="project-entry">
    <h2>Ross School of Business - Green Wolverine Fund Application Adjunct - Market Research [2018]</h2>
    <p><b>What:</b> I aggregated publicly-available data regarding dispensary product offerings and pricing for four Michigan localities:
        Ann Arbor, Bay City, East Lansing, and Flint. I used these aggregates to create the visualizations linked below.</p>
    <p><b>Why:</b> I was interested in joining a student investment-fund and discovered as a non-business major looking to join in winter semester, one of the few options
        available was through the Green Wolverine Fund run out of Ross School of Business, which specialized in cannabis investments. Knowing little about both finance and
        the cannabis sector, as part of the competitive application process I chose to prove my tech skills could be beneficial to the fund by conducting a difficult
        web-scraping job of a specialized domain-specific ecommerce hub where legal operations posted their "menu" of inventory and price-points.</p>
    <p><b>Sidenote:</b> This is also the first scraping project so involved that I had to learn to utilize Selenium's web-driver library to deal with issues like
    javascript-based elemental load-in, scroll-limited load-in, and process-disruptive pop-ups.</p>
    <p><b>Experience:</b></p>
    <ul>
        <li>Visual 1: <a href="https://plot.ly/~mcshaned/12/herb-type-distribution-by-city/#/">
            Herbal Product Type Distribution</a></li>
        <li>Visual 2: <a href="https://plot.ly/~mcshaned/8/herb-effect-distribution-by-city/">
            Distribution by Reported Herbal Product Effect</a></li>
        <li>Visual 3: <a href="https://plot.ly/~mcshaned/14/herb-flavor-distribution-by-city/">
            Distribution of Reported Herbal Product Flavor</a></li>
        <li>Visual 4: <a href="https://plot.ly/~mcshaned/16/inventory-distribution-by-product-type/#/">
            Distribution by Product Category</a></li>
        <li>Visual 5: <a href="https://plot.ly/~mcshaned/18/traditional-herb-vs-alternative-cannabis-products/#/">
            Conventional Dry-Herb Product V. Alternative-Intake Methods (Concentrates, Topicals, Edibles, etc.)</a></li>
    </ul>
    <p>Source Code: Deprecated (numerous structural changes have occurred on the target platform)</p>
    </div>

    </main>

<footer>
</footer>
</body>
</html>
